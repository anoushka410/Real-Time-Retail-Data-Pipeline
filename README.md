# Real-Time Retail Analytics Pipeline

This project demonstrates a real-time data pipeline and analytics dashboard for an online retail dataset. It simulates streaming transactional data, processes it using open-source tools, and visualizes insights through a live dashboard.

---

![alt text](Architecture_diagram.png)

## Approach Taken

1. Simulated real-time ingestion of transactions using **Kafka Producer**.
2. Streamed, validated, and cleaned events using a **Kafka Consumer** (Python).
3. Stored curated data in a **partitioned PostgreSQL** table for efficient querying.
4. Built a **live-updating Streamlit dashboard** with transaction insights.
5. Exposed **custom Prometheus metrics** (success, failure, invalid counts) from the consumer.
6. Sent metrics to **Grafana Cloud** for real-time pipeline monitoring and alerting.

---

## Tech Stack Used

- **Ingestion**: Apache Kafka Producer
- **Processing**: Kafka Consumer (Python, psycopg2)
- **Storage**: PostgreSQL
- **Visualization**: Streamlit (with Matplotlib and Seaborn)
- **Monitoring**: Prometheus + Grafana Cloud

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Dashboard/                   # Streamlit dashboard app
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies for dashboard
â”‚   â”œâ”€â”€ streamlit_local.py       # Local Streamlit app
â”‚   â””â”€â”€ streamlit_app.py         # Streamlit visualization scripts for Streamlit Cloud
â”‚
â”œâ”€â”€ Database/                    # SQL scripts and DB optimizations
â”‚   â”œâ”€â”€ all_queries.sql          # SQL setup (tables, indexes, partitions)
â”‚   â””â”€â”€ optimization.md          # Database optimization notes
â”‚
â”œâ”€â”€ Pipeline/                    # Kafka producer-consumer and ingestion pipeline
â”‚   â”œâ”€â”€ Deprecated/              # Older or backup scripts (archived)
â”‚   â”œâ”€â”€ dlq_records.log          # Dead Letter Queue log file
â”‚   â”œâ”€â”€ kafka_consumer.py        # Kafka consumer with validation, DB inserts, metrics
â”‚   â”œâ”€â”€ kafka_producer.py        # Kafka producer to simulate streaming
â”‚   â”œâ”€â”€ online_retail_data.csv   # Sample dataset for simulation
â”‚   â””â”€â”€ requirements.txt         # Python dependencies for pipeline
â”‚
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ architecture.png             # Pipeline architecture diagram

```

---

###  Setup Documentation: 
https://cyan-braid-e26.notion.site/Real-Time-Retail-Data-Pipeline-1e34070bb67380e6aa31eac5cbe140ac

---

## ğŸš€ Quick Start - Ingestion Pipeline

```bash
# Go to the Code directory
cd Pipeline

# Install Python dependencies
pip install -r requirements.txt

# Run Kafka Producer
python kafka_producer.py

# Run Kafka Consumer (with Prometheus metrics exposed)
python kafka_consumer.py

```

## ğŸš€ Quick Start - Streamlit App (Local)

```bash
# Go to the Code directory
cd Dashboard

# Install Python dependencies
pip install -r requirements.txt

# Run Streamlit app locally
streamlit run streamlit_local.py

---
